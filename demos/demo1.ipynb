{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "##  Examples from  Paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm\n",
    "\n",
    "from torch.distributions.normal import Normal\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from nde.flows import realnvp\n",
    "import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from nde.flows import autoregressive as ar\n",
    "import utils\n",
    "\n",
    "torch.set_default_tensor_type(torch.FloatTensor)\n",
    "\n",
    "\n",
    "def survey_sample(n, centre, rho = 1):\n",
    "    # x = np.random.randn(n) * rho  + centre\n",
    "    x = np.random.uniform(size=n, low =c1 - 3, high=c2 + 3)\n",
    "    y = np.random.uniform(size=n, low =0, high=1)\n",
    "    return torch.FloatTensor(np.concatenate([x.reshape(n, 1), y.reshape(n, 1)], axis=1))\n",
    "\n",
    "def calLoss(inputs, log_prob):\n",
    "    return - (f(inputs)  / log_prob.exp() * log_prob).mean()\n",
    "\n",
    "def filterInputs(inputs):\n",
    "    return inputs[((inputs[:,0]>xlimits[0]) * (inputs[:,0]<xlimits[1]))\n",
    "                  * ((inputs[:,1]>ylimits[0]) * (inputs[:,1]<ylimits[1]))]\n",
    "\n",
    "def filterInputs_logj(inputs, logj):\n",
    "    mask = ((inputs[:,0]>xlimits[0]) * (inputs[:,0]<xlimits[1])) \\\n",
    "           * ((inputs[:,1]>ylimits[0]) * (inputs[:,1]<ylimits[1]))\n",
    "    return inputs[mask], logj[mask]\n",
    "\n",
    "def plotHistory(history, level=10):\n",
    "    history = np.array(history)\n",
    "    idx =  np.where(history > level)[0][-1]\n",
    "    history =  history[idx+1:]\n",
    "    plt.plot(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "Survey:   0%|          | 0/200 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "33396faad4bc44a68f2ac29b74c3ae64"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/John/opt/anaconda3/envs/pytorch/lib/python3.10/site-packages/torch/nn/functional.py:1944: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------\n",
      "0\n",
      "1\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "index -1 is out of bounds for dimension 2 with size 11",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "\u001B[0;32m/var/folders/kb/jnvlsd896tj2g_5wz25rg9t00000gn/T/ipykernel_36072/1167432509.py\u001B[0m in \u001B[0;36m<cell line: 29>\u001B[0;34m()\u001B[0m\n\u001B[1;32m     31\u001B[0m         \u001B[0minputs\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0msurvey_sample\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;36m500\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m3\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     32\u001B[0m         \u001B[0minputs\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mfilterInputs\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0minputs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 33\u001B[0;31m     \u001B[0mlog_prob\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mflow\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mlog_prob\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0minputs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     34\u001B[0m     \u001B[0moptimizer_servy\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mzero_grad\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     35\u001B[0m     \u001B[0mloss\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mcalLoss\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0minputs\u001B[0m\u001B[0;34m,\u001B[0m  \u001B[0mlog_prob\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/Documents/AustraliaStudy/Study/Research/project/demos/Rare-Events/demos/nde/distributions/base.py\u001B[0m in \u001B[0;36mlog_prob\u001B[0;34m(self, inputs, context)\u001B[0m\n\u001B[1;32m     35\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0minputs\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mshape\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m!=\u001B[0m \u001B[0mcontext\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mshape\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     36\u001B[0m                 \u001B[0;32mraise\u001B[0m \u001B[0mValueError\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'Number of input items must be equal to number of context items.'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 37\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_log_prob\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0minputs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcontext\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     38\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     39\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0m_log_prob\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0minputs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcontext\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/Documents/AustraliaStudy/Study/Research/project/demos/Rare-Events/demos/nde/flows/base.py\u001B[0m in \u001B[0;36m_log_prob\u001B[0;34m(self, inputs, context)\u001B[0m\n\u001B[1;32m     22\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     23\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0m_log_prob\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0minputs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcontext\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 24\u001B[0;31m         \u001B[0mnoise\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mlogabsdet\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_transform\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0minputs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcontext\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mcontext\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     25\u001B[0m         \u001B[0mlog_prob\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_distribution\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mlog_prob\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mnoise\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcontext\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mcontext\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     26\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0mlog_prob\u001B[0m \u001B[0;34m+\u001B[0m \u001B[0mlogabsdet\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/opt/anaconda3/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py\u001B[0m in \u001B[0;36m_call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1108\u001B[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001B[1;32m   1109\u001B[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001B[0;32m-> 1110\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mforward_call\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0minput\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1111\u001B[0m         \u001B[0;31m# Do not call functions when jit is used\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1112\u001B[0m         \u001B[0mfull_backward_hooks\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mnon_full_backward_hooks\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/Documents/AustraliaStudy/Study/Research/project/demos/Rare-Events/demos/nde/transforms/base.py\u001B[0m in \u001B[0;36mforward\u001B[0;34m(self, inputs, context)\u001B[0m\n\u001B[1;32m     57\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mforward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0minputs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcontext\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mNone\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     58\u001B[0m         \u001B[0mfuncs\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_transforms\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 59\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_cascade\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0minputs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mfuncs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcontext\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     60\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     61\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0minverse\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0minputs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcontext\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mNone\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/Documents/AustraliaStudy/Study/Research/project/demos/Rare-Events/demos/nde/transforms/base.py\u001B[0m in \u001B[0;36m_cascade\u001B[0;34m(inputs, funcs, context)\u001B[0m\n\u001B[1;32m     51\u001B[0m             \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0midx\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     52\u001B[0m             \u001B[0midx\u001B[0m\u001B[0;34m+=\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 53\u001B[0;31m             \u001B[0moutputs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mlogabsdet\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mfunc\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0moutputs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcontext\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     54\u001B[0m             \u001B[0mtotal_logabsdet\u001B[0m \u001B[0;34m+=\u001B[0m \u001B[0mlogabsdet\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     55\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0moutputs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtotal_logabsdet\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/opt/anaconda3/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py\u001B[0m in \u001B[0;36m_call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1108\u001B[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001B[1;32m   1109\u001B[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001B[0;32m-> 1110\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mforward_call\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0minput\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1111\u001B[0m         \u001B[0;31m# Do not call functions when jit is used\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1112\u001B[0m         \u001B[0mfull_backward_hooks\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mnon_full_backward_hooks\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/Documents/AustraliaStudy/Study/Research/project/demos/Rare-Events/demos/nde/transforms/coupling.py\u001B[0m in \u001B[0;36mforward\u001B[0;34m(self, inputs, context)\u001B[0m\n\u001B[1;32m     76\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     77\u001B[0m         \u001B[0mtransform_params\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtransform_net\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0midentity_split\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcontext\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 78\u001B[0;31m         transform_split, logabsdet = self._coupling_transform_forward(\n\u001B[0m\u001B[1;32m     79\u001B[0m             \u001B[0minputs\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mtransform_split\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     80\u001B[0m             \u001B[0mtransform_params\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mtransform_params\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/Documents/AustraliaStudy/Study/Research/project/demos/Rare-Events/demos/nde/transforms/coupling.py\u001B[0m in \u001B[0;36m_coupling_transform_forward\u001B[0;34m(self, inputs, transform_params)\u001B[0m\n\u001B[1;32m    183\u001B[0m \u001B[0;32mclass\u001B[0m \u001B[0mPiecewiseCouplingTransform\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mCouplingTransform\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    184\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0m_coupling_transform_forward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0minputs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtransform_params\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 185\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_coupling_transform\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0minputs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtransform_params\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0minverse\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mFalse\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    186\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    187\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0m_coupling_transform_inverse\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0minputs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtransform_params\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/Documents/AustraliaStudy/Study/Research/project/demos/Rare-Events/demos/nde/transforms/coupling.py\u001B[0m in \u001B[0;36m_coupling_transform\u001B[0;34m(self, inputs, transform_params, inverse)\u001B[0m\n\u001B[1;32m    198\u001B[0m             \u001B[0mtransform_params\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtransform_params\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mreshape\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mb\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0md\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m-\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    199\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 200\u001B[0;31m         \u001B[0moutputs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mlogabsdet\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_piecewise_cdf\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0minputs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtransform_params\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0minverse\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    201\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    202\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0moutputs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mutils\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msum_except_batch\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mlogabsdet\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/Documents/AustraliaStudy/Study/Research/project/demos/Rare-Events/demos/nde/transforms/coupling.py\u001B[0m in \u001B[0;36m_piecewise_cdf\u001B[0;34m(self, inputs, transform_params, inverse)\u001B[0m\n\u001B[1;32m    461\u001B[0m             }\n\u001B[1;32m    462\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 463\u001B[0;31m         return spline_fn(\n\u001B[0m\u001B[1;32m    464\u001B[0m             \u001B[0minputs\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0minputs\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    465\u001B[0m             \u001B[0munnormalized_widths\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0munnormalized_widths\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/Documents/AustraliaStudy/Study/Research/project/demos/Rare-Events/demos/nde/transforms/splines/rational_quadratic.py\u001B[0m in \u001B[0;36mrational_quadratic_spline\u001B[0;34m(inputs, unnormalized_widths, unnormalized_heights, unnormalized_derivatives, inverse, left, right, bottom, top, min_bin_width, min_bin_height, min_derivative)\u001B[0m\n\u001B[1;32m     97\u001B[0m         \u001B[0mbin_idx\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mutils\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msearchsorted\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcumwidths\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0minputs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m...\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     98\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 99\u001B[0;31m     \u001B[0minput_cumwidths\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mcumwidths\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgather\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m-\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mbin_idx\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m...\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    100\u001B[0m     \u001B[0minput_bin_widths\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mwidths\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgather\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m-\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mbin_idx\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m...\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    101\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mRuntimeError\u001B[0m: index -1 is out of bounds for dimension 2 with size 11"
     ]
    }
   ],
   "source": [
    "flow = realnvp.SimpleRealNVP(\n",
    "    features=2,\n",
    "    hidden_features=20,\n",
    "    num_layers=15,\n",
    "    num_blocks_per_layer=2,\n",
    ")\n",
    "\n",
    "# flow = ar.MaskedAutoregressiveFlow(\n",
    "#             features=2,\n",
    "#             hidden_features=30,\n",
    "#             num_layers=20,\n",
    "#             num_blocks_per_layer=2,\n",
    "#         )\n",
    "\n",
    "\n",
    "c1 = 1\n",
    "c2 = 15\n",
    "centre = (c1 + c2 )/2\n",
    "xlimits = [-50, 200]\n",
    "ylimits = [0, 1]\n",
    "myNorm1 = Normal(torch.tensor([c1]), torch.tensor([0.5]))\n",
    "myNorm2 = Normal(torch.tensor([c2]), torch.tensor([1.0]))\n",
    "def f(x):\n",
    "    return (x[:,1]>0) * (x[:,1]<1) * (myNorm1.log_prob(x[:,0]).exp() * 0.7 + myNorm2.log_prob(x[:,0]).exp() * 0.3)\n",
    "\n",
    "\n",
    "optimizer_servy = optim.Adam(flow.parameters(), lr=1e-3)\n",
    "history = []\n",
    "for epoch in tqdm.notebook.tqdm(range(200), desc='Survey', leave=False):\n",
    "    with torch.no_grad():\n",
    "        inputs = survey_sample(500, 3)\n",
    "        inputs = filterInputs(inputs)\n",
    "    log_prob = flow.log_prob(inputs)\n",
    "    optimizer_servy.zero_grad()\n",
    "    loss = calLoss(inputs,  log_prob)\n",
    "    loss.backward()\n",
    "    optimizer_servy.step()\n",
    "    history.append(loss.item())\n",
    "\n",
    "print(\"===>\", inputs[:,0].mean(), loss.item())\n",
    "optimizer_refine = optim.Adam(flow.parameters(), lr=2e-4)\n",
    "for epoch in tqdm.notebook.tqdm(range(1000), desc='Refine', leave=False):\n",
    "    with torch.no_grad():\n",
    "        inputs = flow.sample(2000).detach()\n",
    "        inputs = filterInputs(inputs)\n",
    "        if(epoch % 50 == 49):\n",
    "            print(inputs[:,0].mean(), loss.item())\n",
    "    log_prob = flow.log_prob(inputs)\n",
    "    optimizer_refine.zero_grad()\n",
    "    loss =  calLoss(inputs,  log_prob)\n",
    "    loss.backward()\n",
    "    optimizer_refine.step()\n",
    "    history.append(loss.item())\n",
    "\n",
    "plotHistory(history, 15)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "with torch.no_grad():\n",
    "    samples = flow.sample(5000)\n",
    "    samples = filterInputs(samples)\n",
    "    s0, s1 = samples[:,0], samples[:,1]\n",
    "\n",
    "    print(s0.mean())\n",
    "    plt.figure(figsize=(10,5))\n",
    "    plt.subplot(121)\n",
    "    plt.scatter(s0, s1, marker='o', alpha=0.1)\n",
    "    plt.plot(0, 0, 'rp', markersize=5)\n",
    "\n",
    "    plt.title(\"2d\")\n",
    "    plt.subplot(122)\n",
    "    plt.hist((s0).detach().numpy(), bins=50,  range=(c1-8,  c2+8), density=True) #, range=(0,  200)\n",
    "    plt.title('1d')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "Survey:   0%|          | 0/200 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d1c5c18088cb4c4bacc67f4a17674a39"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "---------------\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n"
     ]
    },
    {
     "ename": "InputOutsideDomain",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mInputOutsideDomain\u001B[0m                        Traceback (most recent call last)",
      "\u001B[0;32m/var/folders/kb/jnvlsd896tj2g_5wz25rg9t00000gn/T/ipykernel_36072/2845039868.py\u001B[0m in \u001B[0;36m<cell line: 35>\u001B[0;34m()\u001B[0m\n\u001B[1;32m     38\u001B[0m         \u001B[0minputs\u001B[0m  \u001B[0;34m=\u001B[0m \u001B[0mfilterInputs\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0minputs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     39\u001B[0m         \u001B[0;31m# print(inputs.shape)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 40\u001B[0;31m     \u001B[0mlog_prob\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mflow\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mlog_prob\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0minputs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     41\u001B[0m     \u001B[0moptimizer_servy\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mzero_grad\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     42\u001B[0m     \u001B[0mloss\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mcalLoss\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0minputs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mlog_prob\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/Documents/AustraliaStudy/Study/Research/project/demos/Rare-Events/demos/nde/distributions/base.py\u001B[0m in \u001B[0;36mlog_prob\u001B[0;34m(self, inputs, context)\u001B[0m\n\u001B[1;32m     35\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0minputs\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mshape\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m!=\u001B[0m \u001B[0mcontext\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mshape\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     36\u001B[0m                 \u001B[0;32mraise\u001B[0m \u001B[0mValueError\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'Number of input items must be equal to number of context items.'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 37\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_log_prob\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0minputs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcontext\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     38\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     39\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0m_log_prob\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0minputs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcontext\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/Documents/AustraliaStudy/Study/Research/project/demos/Rare-Events/demos/nde/flows/base.py\u001B[0m in \u001B[0;36m_log_prob\u001B[0;34m(self, inputs, context)\u001B[0m\n\u001B[1;32m     22\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     23\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0m_log_prob\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0minputs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcontext\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 24\u001B[0;31m         \u001B[0mnoise\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mlogabsdet\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_transform\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0minputs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcontext\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mcontext\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     25\u001B[0m         \u001B[0mlog_prob\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_distribution\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mlog_prob\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mnoise\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcontext\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mcontext\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     26\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0mlog_prob\u001B[0m \u001B[0;34m+\u001B[0m \u001B[0mlogabsdet\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/opt/anaconda3/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py\u001B[0m in \u001B[0;36m_call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1108\u001B[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001B[1;32m   1109\u001B[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001B[0;32m-> 1110\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mforward_call\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0minput\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1111\u001B[0m         \u001B[0;31m# Do not call functions when jit is used\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1112\u001B[0m         \u001B[0mfull_backward_hooks\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mnon_full_backward_hooks\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/Documents/AustraliaStudy/Study/Research/project/demos/Rare-Events/demos/nde/transforms/base.py\u001B[0m in \u001B[0;36mforward\u001B[0;34m(self, inputs, context)\u001B[0m\n\u001B[1;32m     57\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mforward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0minputs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcontext\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mNone\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     58\u001B[0m         \u001B[0mfuncs\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_transforms\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 59\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_cascade\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0minputs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mfuncs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcontext\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     60\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     61\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0minverse\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0minputs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcontext\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mNone\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/Documents/AustraliaStudy/Study/Research/project/demos/Rare-Events/demos/nde/transforms/base.py\u001B[0m in \u001B[0;36m_cascade\u001B[0;34m(inputs, funcs, context)\u001B[0m\n\u001B[1;32m     51\u001B[0m             \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0midx\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     52\u001B[0m             \u001B[0midx\u001B[0m\u001B[0;34m+=\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 53\u001B[0;31m             \u001B[0moutputs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mlogabsdet\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mfunc\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0moutputs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcontext\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     54\u001B[0m             \u001B[0mtotal_logabsdet\u001B[0m \u001B[0;34m+=\u001B[0m \u001B[0mlogabsdet\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     55\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0moutputs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtotal_logabsdet\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/opt/anaconda3/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py\u001B[0m in \u001B[0;36m_call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1108\u001B[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001B[1;32m   1109\u001B[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001B[0;32m-> 1110\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mforward_call\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0minput\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1111\u001B[0m         \u001B[0;31m# Do not call functions when jit is used\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1112\u001B[0m         \u001B[0mfull_backward_hooks\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mnon_full_backward_hooks\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/Documents/AustraliaStudy/Study/Research/project/demos/Rare-Events/demos/nde/transforms/base.py\u001B[0m in \u001B[0;36mforward\u001B[0;34m(self, inputs, context)\u001B[0m\n\u001B[1;32m    216\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    217\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mforward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0minputs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcontext\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mNone\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 218\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_transform\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0minverse\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0minputs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcontext\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    219\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    220\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0minverse\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0minputs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcontext\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mNone\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/Documents/AustraliaStudy/Study/Research/project/demos/Rare-Events/demos/nde/transforms/nonlinearities.py\u001B[0m in \u001B[0;36minverse\u001B[0;34m(self, inputs, context)\u001B[0m\n\u001B[1;32m    125\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0minverse\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0minputs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcontext\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mNone\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    126\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mtorch\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmin\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0minputs\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m<\u001B[0m \u001B[0;36m0\u001B[0m \u001B[0;32mor\u001B[0m \u001B[0mtorch\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmax\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0minputs\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m>\u001B[0m \u001B[0;36m1\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 127\u001B[0;31m             \u001B[0;32mraise\u001B[0m \u001B[0mtransforms\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mInputOutsideDomain\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    128\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    129\u001B[0m         \u001B[0minputs\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtorch\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mclamp\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0minputs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0meps\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m1\u001B[0m \u001B[0;34m-\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0meps\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mInputOutsideDomain\u001B[0m: "
     ]
    }
   ],
   "source": [
    "flow = realnvp.SimpleRealNVP(\n",
    "    features=2,\n",
    "    hidden_features=20,\n",
    "    num_layers=8,\n",
    "    num_blocks_per_layer=2,\n",
    ")\n",
    "\n",
    "# flow = ar.MaskedAutoregressiveFlow(\n",
    "#             features=2,\n",
    "#             hidden_features=30,\n",
    "#             num_layers=20,\n",
    "#             num_blocks_per_layer=2,\n",
    "#         )\n",
    "\n",
    "centre = 0.0\n",
    "level = -0\n",
    "xlimits = [-20, 20]\n",
    "ylimits = [0, 1]\n",
    "\n",
    "myNorm = Normal(torch.tensor([centre]), torch.tensor([1.0]))\n",
    "def logf(x):\n",
    "    return myNorm.log_prob(x[:,0]) + torch.log(x[:,0]>level) \\\n",
    "             + torch.log(x[:,1]>0) + torch.log(x[:,1]<1)\n",
    "\n",
    "def survey_sample(n, centre, rho = 1):\n",
    "    x = np.random.randn(n) * rho  + centre\n",
    "    y = np.random.uniform(size=n, low =0, high=1)\n",
    "    return torch.FloatTensor(np.concatenate([x.reshape(n, 1), y.reshape(n, 1)], axis=1))\n",
    "\n",
    "def calLoss(inputs, log_prob):\n",
    "    return - ((logf(inputs) - log_prob).exp() * log_prob).mean()\n",
    "\n",
    "optimizer_servy = optim.Adam(flow.parameters(), lr=1e-2)\n",
    "history = []\n",
    "for epoch in tqdm.notebook.tqdm(range(200), desc='Survey', leave=False):\n",
    "    with torch.no_grad():\n",
    "        inputs = survey_sample(1000, 0)\n",
    "        inputs  = filterInputs(inputs)\n",
    "        # print(inputs.shape)\n",
    "    log_prob = flow.log_prob(inputs)\n",
    "    optimizer_servy.zero_grad()\n",
    "    loss = calLoss(inputs, log_prob)\n",
    "    loss.backward()\n",
    "    if loss == torch.nan:\n",
    "        break\n",
    "    optimizer_servy.step()\n",
    "    history.append(loss.item())\n",
    "\n",
    "print(\"===>\", inputs[:,0].mean(), loss.item())\n",
    "optimizer_refine = optim.Adam(flow.parameters(), lr=2e-4)\n",
    "for epoch in tqdm.notebook.tqdm(range(500), desc='Refine', leave=False):\n",
    "    with torch.no_grad():\n",
    "        inputs = flow.sample(1000).detach()\n",
    "        inputs  = filterInputs(inputs)\n",
    "\n",
    "        if(epoch % 100 == 99):\n",
    "            print(inputs[:,0].mean(), loss.item())\n",
    "    log_prob = flow.log_prob(inputs)\n",
    "    optimizer_refine.zero_grad()\n",
    "    loss = calLoss(inputs, log_prob)\n",
    "    if loss == torch.nan:\n",
    "        break\n",
    "    loss.backward()\n",
    "    optimizer_refine.step()\n",
    "    history.append(loss.item())\n",
    "\n",
    "plotHistory(history, 5)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "with torch.no_grad():\n",
    "    x, loggx = flow.sample_and_log_prob(10000)\n",
    "    x, loggx = filterInputs_logj(x, loggx)\n",
    "    # print(x.shape)\n",
    "    s0, s1 = x[:,0], x[:,1]\n",
    "\n",
    "    intgral = torch.exp(logf(x) - loggx).mean()\n",
    "\n",
    "    print(\"==integral=>\", intgral)\n",
    "    plt.figure(figsize=(10,5))\n",
    "    plt.subplot(121)\n",
    "    plt.scatter(s0, s1, marker='o', alpha=0.1)\n",
    "    plt.plot(0, 0, 'rp', markersize=5)\n",
    "\n",
    "    plt.title(\"2d\")\n",
    "    plt.subplot(122)\n",
    "    plt.hist((s0).detach().numpy(), bins=100,  range=(-10,  10), density=True) #, range=(0,  200)\n",
    "    plt.title('1d')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    x, loggx = flow.sample_and_log_prob(10000)\n",
    "    x, loggx = filterInputs_logj(x, loggx)\n",
    "    s0, s1 = x[:,0], x[:,1]\n",
    "\n",
    "    intgral = torch.exp(logf(x) - loggx).mean()\n",
    "\n",
    "    print(\"==integral=>\", intgral)\n",
    "    plt.figure(figsize=(10,5))\n",
    "    plt.subplot(121)\n",
    "    plt.scatter(s0, s1, marker='o', alpha=0.1)\n",
    "    plt.plot(0, 0, 'rp', markersize=5)\n",
    "\n",
    "    plt.title(\"2d\")\n",
    "    plt.subplot(122)\n",
    "    plt.hist((s0).detach().numpy(), bins=100,  range=(level-10,  level+20), density=True) #, range=(0,  200)\n",
    "    plt.title('1d')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "import matplotlib.pyplot as plt\n",
    "1-norm.cdf(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1)\n",
    "x = np.linspace(0, 5, 100)\n",
    "ax.plot(x, norm.pdf(x),\n",
    "       'r-', lw=5, alpha=0.6, label='norm pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "$\\{\\boldsymbol{X}_{t},t\\in\\mathcal{T}\\} $\n",
    "\n",
    "$\\boldsymbol{R}_{T}=\\sum_{i=1}^{T}\\boldsymbol{X}_{t}$\n",
    "\n",
    "$\\{\\boldsymbol{R}_{T}\\leq\\gamma\\}$\n",
    "\n",
    "$P\\{y_{t+k}=1|\\boldsymbol{X}_{t}\\}$\n",
    "\n",
    "$\\boldsymbol{X}$\n",
    "\n",
    "$\\boldsymbol{X}_{t-}=\\{\\boldsymbol{X}_{t},\\boldsymbol{X}_{t-1},\\cdots\\}$"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    x, loggx = flow.sample_and_log_prob(10000)\n",
    "    x, loggx = filterInputs_logj(x, loggx)\n",
    "    s0, s1 = x[:,0], x[:,1]\n",
    "\n",
    "    intgral = torch.exp(logf(x) - loggx).mean()\n",
    "\n",
    "    print(intgral)\n",
    "    plt.figure(figsize=(10,5))\n",
    "    plt.subplot(121)\n",
    "    plt.scatter(s0, s1, marker='o', alpha=0.1)\n",
    "    plt.plot(0, 0, 'rp', markersize=5)\n",
    "\n",
    "    plt.title(\"2d\")\n",
    "    plt.subplot(122)\n",
    "    plt.hist((s0).detach().numpy(), bins=100,  range=(level-10,  level+20), density=True) #, range=(0,  200)\n",
    "    plt.title('1d')\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "flow = realnvp.SimpleRealNVP(\n",
    "    features=2,\n",
    "    hidden_features=20,\n",
    "    num_layers=10,\n",
    "    num_blocks_per_layer=2,\n",
    ")\n",
    "\n",
    "# flow = ar.MaskedAutoregressiveFlow(\n",
    "#             features=2,\n",
    "#             hidden_features=30,\n",
    "#             num_layers=20,\n",
    "#             num_blocks_per_layer=2,\n",
    "#         )\n",
    "\n",
    "centre = 0\n",
    "level = 10\n",
    "centre_servey =  level + 0.2\n",
    "\n",
    "xlimits = [0, 30]\n",
    "ylimits = [0, 30]\n",
    "\n",
    "def f(x):\n",
    "    return torch.FloatTensor(np.exp(-x[:, 0] -x[:, 1])) * (x[:,0] + x[:,1] > level) * (x[:,0]>0)* (x[:,1]>0)\n",
    "\n",
    "def logf(x):\n",
    "      return -x[:, 0] -x[:, 1] + torch.log(x[:,0] + x[:,1] > level) \\\n",
    "             + torch.log(x[:,0]>0) + torch.log(x[:,1]>0)\n",
    "\n",
    "def calLoss(inputs, log_prob):\n",
    "    return - ((logf(inputs) - log_prob).exp() * log_prob).mean()\n",
    "\n",
    "def survey_sample(n, centre, rho = 1):\n",
    "    x = np.random.uniform(size=n, low =0, high=level*1.5)\n",
    "    y = np.random.uniform(size=n, low =0, high=level*1.5)\n",
    "    return torch.FloatTensor(np.concatenate([x.reshape(n, 1), y.reshape(n, 1)], axis=1))\n",
    "\n",
    "optimizer_servy = optim.Adam(flow.parameters(), lr=1e-3)\n",
    "history = []\n",
    "for epoch in tqdm.notebook.tqdm(range(200), desc='Survey', leave=False):\n",
    "    with torch.no_grad():\n",
    "        inputs = survey_sample(1000, centre_servey)\n",
    "        inputs =  filterInputs(inputs)\n",
    "    log_prob = flow.log_prob(inputs)\n",
    "    optimizer_servy.zero_grad()\n",
    "    loss = calLoss(inputs, log_prob)\n",
    "    # loss = - (f(inputs) * log_prob / log_prob.exp() ).mean() #  (log_prob/logf(inputs) ).mean()\n",
    "    loss.backward()\n",
    "    optimizer_servy.step()\n",
    "    history.append(loss.item())\n",
    "\n",
    "print(\"===>\", inputs[:,0].mean(), loss.item())\n",
    "optimizer_refine = optim.Adam(flow.parameters(), lr=1e-3)\n",
    "for epoch in tqdm.notebook.tqdm(range(1000), desc='Refine', leave=False):\n",
    "    with torch.no_grad():\n",
    "        inputs = flow.sample(800).detach()\n",
    "        inputs =  filterInputs(inputs)\n",
    "        if(epoch % 50 == 49):\n",
    "            print(inputs[:,0].mean(), loss.item())\n",
    "    log_prob = flow.log_prob(inputs)\n",
    "    optimizer_refine.zero_grad()\n",
    "    loss = calLoss(inputs, log_prob)\n",
    "    loss.backward()\n",
    "    optimizer_refine.step()\n",
    "    history.append(loss.item())\n",
    "\n",
    "plotHistory(history, 0.01)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "with torch.no_grad():\n",
    "    samples = flow.sample(3000)\n",
    "    samples = filterInputs(samples)\n",
    "    s0, s1 = samples[:,0], samples[:,1]\n",
    "    print(s0.mean())\n",
    "    plt.figure(figsize=(10,5))\n",
    "    plt.subplot(121)\n",
    "    plt.scatter(s0, s1, marker='o', alpha=0.1)\n",
    "    plt.plot(0, 0, 'rp', markersize=5)\n",
    "\n",
    "    plt.title(\"2d\")\n",
    "    plt.subplot(122)\n",
    "    plt.hist((s0+s1).detach().numpy(), bins=100,  range=(level-10,  level+20), density=True) #, range=(0,  200)\n",
    "    plt.title('1d')\n",
    "    plt.show()\n",
    "\n",
    "    def myexp(x):\n",
    "        return np.exp(-x) *  (x>10)\n",
    "    x = np.linspace(0,20,100)\n",
    "    plt.plot(x,  myexp(x))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    samples = flow.sample(10000)\n",
    "    samples = filterInputs(samples)\n",
    "    s0, s1 = samples[:,0], samples[:,1]\n",
    "    print(s0.mean())\n",
    "    plt.figure(figsize=(10,5))\n",
    "    plt.subplot(121)\n",
    "    plt.scatter(s0, s1, marker='o', alpha=0.1)\n",
    "    plt.plot(0, 0, 'rp', markersize=5)\n",
    "\n",
    "    plt.title(\"2d\")\n",
    "    plt.subplot(122)\n",
    "    plt.hist((s0+s1).detach().numpy(), bins=100,  range=(level-10,  level+20), density=True) #, range=(0,  200)\n",
    "    plt.title('1d')\n",
    "    plt.show()\n",
    "\n",
    "    def myexp(x):\n",
    "        return np.exp(-x) *  (x>10)\n",
    "    x = np.linspace(0,20,100)\n",
    "    plt.plot(x,  myexp(x))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "flow = realnvp.SimpleRealNVP(\n",
    "    features=2,\n",
    "    hidden_features=20,\n",
    "    num_layers=10,\n",
    "    num_blocks_per_layer=2,\n",
    ")\n",
    "\n",
    "# flow = ar.MaskedAutoregressiveFlow(\n",
    "#             features=2,\n",
    "#             hidden_features=30,\n",
    "#             num_layers=20,\n",
    "#             num_blocks_per_layer=2,\n",
    "#         )\n",
    "\n",
    "centre = 0\n",
    "level = 10\n",
    "centre_servey =  level + 0.2\n",
    "\n",
    "xlimits = [0, 30]\n",
    "ylimits = [0, 30]\n",
    "\n",
    "def f(x):\n",
    "    return torch.FloatTensor(np.exp(-x[:, 0] -x[:, 1])) * (x[:,0] + x[:,1] > level) * (x[:,0]>0)* (x[:,1]>0)\n",
    "\n",
    "def logf(x):\n",
    "      return -x[:, 0] -x[:, 1] + torch.log(x[:,0] + x[:,1] > level) \\\n",
    "             + torch.log(x[:,0]>0) + torch.log(x[:,1]>0)\n",
    "\n",
    "def calLoss(inputs, log_prob):\n",
    "    return - ((logf(inputs) - log_prob).exp() * log_prob).mean()\n",
    "\n",
    "def survey_sample(n, centre, rho = 1):\n",
    "    x = np.random.uniform(size=n, low =0, high=level*1.5)\n",
    "    y = np.random.uniform(size=n, low =0, high=level*1.5)\n",
    "    return torch.FloatTensor(np.concatenate([x.reshape(n, 1), y.reshape(n, 1)], axis=1))\n",
    "\n",
    "optimizer_servy = optim.Adam(flow.parameters(), lr=1e-3)\n",
    "history = []\n",
    "for epoch in tqdm.notebook.tqdm(range(200), desc='Survey', leave=False):\n",
    "    with torch.no_grad():\n",
    "        inputs = survey_sample(1000, centre_servey)\n",
    "        inputs =  filterInputs(inputs)\n",
    "    log_prob = flow.log_prob(inputs)\n",
    "    optimizer_servy.zero_grad()\n",
    "    loss = calLoss(inputs, log_prob)\n",
    "    # loss = - (f(inputs) * log_prob / log_prob.exp() ).mean() #  (log_prob/logf(inputs) ).mean()\n",
    "    loss.backward()\n",
    "    optimizer_servy.step()\n",
    "    history.append(loss.item())\n",
    "\n",
    "print(\"===>\", inputs[:,0].mean(), loss.item())\n",
    "optimizer_refine = optim.Adam(flow.parameters(), lr=1e-3)\n",
    "for epoch in tqdm.notebook.tqdm(range(1000), desc='Refine', leave=False):\n",
    "    with torch.no_grad():\n",
    "        inputs = flow.sample(800).detach()\n",
    "        inputs =  filterInputs(inputs)\n",
    "        if(epoch % 50 == 49):\n",
    "            print(inputs[:,0].mean(), loss.item())\n",
    "    log_prob = flow.log_prob(inputs)\n",
    "    optimizer_refine.zero_grad()\n",
    "    loss = calLoss(inputs, log_prob)\n",
    "    loss.backward()\n",
    "    optimizer_refine.step()\n",
    "    history.append(loss.item())\n",
    "\n",
    "plotHistory(history, 0.01)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "with torch.no_grad():\n",
    "    samples = flow.sample(3000)\n",
    "    samples = filterInputs(samples)\n",
    "    s0, s1 = samples[:,0], samples[:,1]\n",
    "    print(s0.mean())\n",
    "    plt.figure(figsize=(10,5))\n",
    "    plt.subplot(121)\n",
    "    plt.scatter(s0, s1, marker='o', alpha=0.1)\n",
    "    plt.plot(0, 0, 'rp', markersize=5)\n",
    "\n",
    "    plt.title(\"2d\")\n",
    "    plt.subplot(122)\n",
    "    plt.hist((s0+s1).detach().numpy(), bins=100,  range=(level-10,  level+20), density=True) #, range=(0,  200)\n",
    "    plt.title('1d')\n",
    "    plt.show()\n",
    "\n",
    "    def myexp(x):\n",
    "        return np.exp(-x) *  (x>10)\n",
    "    x = np.linspace(0,20,100)\n",
    "    plt.plot(x,  myexp(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    samples = flow.sample(10000)\n",
    "    samples = filterInputs(samples)\n",
    "    s0, s1 = samples[:,0], samples[:,1]\n",
    "    print(s0.mean())\n",
    "    plt.figure(figsize=(10,5))\n",
    "    plt.subplot(121)\n",
    "    plt.scatter(s0, s1, marker='o', alpha=0.1)\n",
    "    plt.plot(0, 0, 'rp', markersize=5)\n",
    "\n",
    "    plt.title(\"2d\")\n",
    "    plt.subplot(122)\n",
    "    plt.hist((s0+s1).detach().numpy(), bins=100,  range=(level-10,  level+20), density=True) #, range=(0,  200)\n",
    "    plt.title('1d')\n",
    "    plt.show()\n",
    "\n",
    "    def myexp(x):\n",
    "        return np.exp(-x) *  (x>10)\n",
    "    x = np.linspace(0,20,100)\n",
    "    plt.plot(x,  myexp(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}