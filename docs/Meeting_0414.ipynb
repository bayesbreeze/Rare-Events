{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Meeting0414"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1 Background"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 MLE: The method to fit the density model \n",
    "\n",
    "MLE (Maximum Likelihood Estimation)\n",
    "\n",
    "$$\\underset{\\theta}{\\max}\\sum_{i}\\log p_{\\theta}(x^{(i)})$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Change of Variablles\n",
    "\n",
    "$$z=f_{\\theta}(x)$$\n",
    "$$p_{\\theta}(x)=p(f_{\\theta}(x))\\left|\\cfrac{\\partial f_{\\theta}(x)}{\\partial x}\\right|$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 1.3 Train one-dimensional Normalizing Flows (NFs)\n",
    "\n",
    "The objective funciton:   \n",
    "$$\\underset{\\theta}{\\max}\\sum_{i}\\log p_{\\theta}(x^{(i)})=\\underset{\\theta}{\\max}\\sum_{i}\\log p_{Z}(f_{\\theta}(x^{(i)}))+\\log\\left|\\cfrac{\\partial f_{\\theta}}{\\partial x}(x^{(i)})\\right| $$\n",
    "\n",
    "Note:  \n",
    "1. $f_{\\theta}$ should be **invertiblle & differentiable**, and we usually construct a special CDF as  $f_{\\theta}$, see examples in 2.3\n",
    "2. This objective function is not used in our project, just used here to demonstrate what a common implementation looks like.\n",
    "\n",
    "The following is a one-dimension example. The first figure is the target distribution, the middle is the transformation function, and the last one is a chosen distribution, here is the `Uniform` distribution.\n",
    "\n",
    "<center><img src=\"imgs/m0414a.jpg\" width=600> </img> </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When training the NFs model, we use samples from the target distribution as the input and train a model to find the transformation function using the objective function defined above.\n",
    "\n",
    "Note:\n",
    "\n",
    "We cannot apply this training process to our project directly, because we didn't have samples from the target distribution. The solution is to construct a new objective function described in Müller's paper and will explain it next."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 1.4 Sampling from NFs\n",
    "\n",
    "Step1: sample $z\\sim p_{Z}(z)$   \n",
    "Step2: $x=f_{\\theta}^{-1}(z)$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 2. Using NFs in Rare-Events Simulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Why NFs is useful for our project? \n",
    "\n",
    "We can treat Normalizing Flows(NFs) as a whole, and it acts as a distribution like Gaussian, they have nice properties. We **can sample and perform density estimation easily**. What's more, NFs are more flexible than Gaussian because NFs **can learn almost any complex distribution**. Therefore, we can use NFs models to replace the Generating Network `G` of our project, and automatically achieve the functionalities of sampling and density estimation. As a result, we can omit the Density Estimation Network of our old implementation. \n",
    "\n",
    "To learn a given target distribution, we can use KL to derive a training objective function. After finished training, we will achieve an NFs model that is very close to the target function and can be very easily handled (easy for sampling and density estimation). Following is the derivation of the objective function.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 2.2 Derivation of the Objective Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we need to estimate the distance between target distribution $p$ and the distribution  $q_{\\theta}$ of samples generated by NFs. We use the KL, and its gradient is:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\begin{align}\\nabla_{\\theta}D_{\\mathbb{KL}}(p||q;\\theta)\t&=\\nabla_{\\theta}\\int_{\\Omega}p(x)\\log\\cfrac{p(x)}{q(x;\\theta)}dx \\\\\n",
    "\t&=\\nabla_{\\theta}\\int_{\\Omega}p(x)\\log p(x)dx-\\nabla_{\\theta}\\int_{\\Omega}p(x)\\log q(x;\\theta)dx \\\\\n",
    "\t&=-\\nabla_{\\theta}\\int_{\\Omega}p(x)\\log q(x;\\theta)dx \\\\\n",
    "\t&=-\\nabla_{\\theta}\\mathbb{\\mathbb{E}}_{x\\sim q(X;\\theta)}\\cfrac{p(X)}{q(X;\\theta)}\\log q(X;\\theta)\\end{align}$$\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is clear that the gradident is only depend on the Cross-Entropy part.\n",
    "\n",
    "Therefore, the objective function is:\n",
    "\n",
    "$$\\begin{align}\n",
    "\\underset{\\theta}{\\min}D_{\\mathbb{KL}}(p||q;\\theta)&=\\underset{\\theta}{\\min}-\\mathbb{\\mathbb{E}}_{x\\sim q(X;\\theta)}\\cfrac{p(X)}{q(X;\\theta)}\\log q(X;\\theta) \\\\\n",
    "&=\\underset{\\theta}{\\min}-\\mathbb{\\mathbb{E}}_{x\\sim q(X;\\theta)}\\cfrac{f(X)/c}{q(X;\\theta)}\\log q(X;\\theta) \\\\\n",
    "&=\\underset{\\theta}{\\min}-\\mathbb{\\mathbb{E}}_{x\\sim q(X;\\theta)}\\cfrac{f(X)}{q(X;\\theta)}\\log q(X;\\theta)\n",
    "\t\\end{align}$$\n",
    "\n",
    "where $c$ is the normalizing constant, $f(X)$ is the target function, $q(X;\\theta)$ can be estimated using flow model, and the samples are generated from the flow model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 2.2 Deal with high dimension -- solve the correlation issue\n",
    "\n",
    "If we assume every dimension is **independent**, this can **simplify** the implementation because the Jacobian matrice is diagonal, and the determinants can be calculated easily. However, this method is **error-prone**, because it is very likely that variables correlate with each other. The solution is **to use a Neural network to learn the correlation between variables**. The popular implementations are Coupling Flows and Autoregressive Flows.\n",
    "\n",
    "\n",
    "The following figure demonstrates the training of Coupling Flow, the $m(x^A)$ is trained by a neural network. **This method solved the correlation problem, but still did not affect the simplicity of the implementation.**\n",
    "\n",
    "<img src=\"imgs/m0414b.jpg\" width=\"600\" > </img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 variety of $f_\\theta$  -- more about accuracy\n",
    "\n",
    "Improving the flexibility of the transformation function can improve the accuracy of the transformation of each dimension. Following are two implementations:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"imgs/m0414c.jpg\" width=\"500\" > </img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 3. What I want to do next?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, I will fix the issue of my current implementation and applied this method to the 1d examples of our paper. \n",
    "\n",
    "Next, I will understand Lachlan's new solution, and try to figure out its relationship with NFs.\n",
    "\n",
    "Finally, I will compare the two methods using the examples of papers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 4. About the communication in the following serveral weeks\n",
    "\n",
    "I will update my work on GitHub and share the link with you. I think this will be more efficient and clear. \n",
    "\n",
    "If I have significant progress, I will notify you via email."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Reference\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. T. Müller, B. McWilliams, F. Rousselle, M. Gross, and J. Novák. “Neural importance sampling”. In: ACM Transactions on Graphics 38.5 (2019), p. 145.\n",
    "\n",
    "2. https://drive.google.com/file/d/1j-3ErOVr8gPLEbN6J4jBeO84I7CqQdde/view"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
